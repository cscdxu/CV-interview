###1.池化的作用是什么？
（1） invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)
（2） 保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力

2.神经网络的损失函数为什么是非凸的?
主要思路是,任意的凸函数的最优点是唯一的. 假设一个最优点A后,如果总能找到另一点B,使AB点的损失值相等, 辅以证明AB不是同一点, 那么就能说明原函数是非凸的了。Quora上有相关的证明：https://www.quora.com/How-can-you-prove-that-the-loss-functions-in-Deep-Neural-nets-are-non-convex

3.反向传播思想：
计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，根据梯度方向更新权值。
（1）将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；
（2）由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。

4.DBN的训练过程
整个网络看成是多个RBM的堆叠，先使用无监督逐层训练，首先训练第一层，然后将第一层预训练好的隐结点视为第二层的输入节点，对第二层进行预训练，各层预训练完成后，再用BP算法对整个网络进行训练。

5.为什么提出CNN？
全连接的结构下会引起参数数量的膨胀，容易过拟合且局部最优。图像中有固有的局部模式可以利用，所以，提出了CNN，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在所有图像内都是共享的，图像通过卷积操作后仍然保留原来的位置关系。

6.CNN的优点
（1）局部感知：一般认为图像的空间联系是局部的像素联系比较密切，而距离较远的像素相关性较弱，因此，每个神经元没必要对全局图像进行感知，只要对局部进行感知，然后在更高层将局部的信息综合起来得到全局信息。利用卷积层实现：（特征映射，每个特征映射是一个神经元阵列）：从上一层通过局部卷积滤波器提取局部特征。卷积层紧跟着一个用来求局部平均与二次提取的计算层，这种二次特征提取结构减少了特征分辨率。
（2）参数共享：在局部连接中，每个神经元的参数都是一样的，即：同一个卷积核在图像中都是共享的。（理解：卷积操作实际是在提取一个个局部信息，而局部信息的一些统计特性和其他部分是一样的，也就意味着这部分学到的特征也可以用到另一部分上。所以对图像上的所有位置，都能使用同样的学习特征。）卷积核共享有个问题：提取特征不充分，可以通过增加多个卷积核来弥补，可以学习多种特征。
（3）采样(池化)层：在通过卷积得到特征后，希望利用这些特征进行分类。基于局部相关性原理进行亚采样，在减少数据量的同时保留有用信息。（压缩数据和参数的量，减少过拟合）（max-polling 和average-polling）
（4）对平移、比例缩放、倾斜或其他形式的变形具有高度不变性。

7.CNN常见的问题
（1）梯度消失问题：过多的层数会导致梯度消失
解决手段：减少层数；增大学习率；用Relu代替sigmoid。
（2）权重衰减：CNN的权重共享相当于自带某种正则项，所以代价函数里可不加正则
（3）随机梯度下降的参数选择，如batch

8.batch的选择
如果数据集比较小，可以采用全数据集的形式，好处：全数据集确定的方向能够更好的代表样本总体；不同权重的梯度值差别巨大，因此选一个全局的学习率很困难，使用全数据集可以只基于梯度符号并且针对性单独更新各权值。
如果数据集比较大，全数据集不可行，内存限制；由于各个batch采样的差异性，各次梯度修正值相互抵消，无法修正。另一个极端每次只训练一个样本，batch=1，每次修正方向以各自样本的梯度方向修正，难以达到收敛。

9.DBN与CNN两者异同
异：DBN：全连接，有pre-train过程；CNN：局部连接，没有预训练过程，但加了卷积，池化。
同：无论是DBN还是CNN，这种多隐层堆叠，每层对上一层的输出进行处理的机制，可看作是在对输入信号进行逐层加工，从而把初始的、与输出目标之间联系不大的输入表示，转化成与输出目标联系密切的表示。即：通过多层处理，逐渐将初始的低层特征表示转化成高层的特征表示后，用“简单模型”就可以完成复杂的分类等学习任务。

10.RNN的提出和原理
DNN存在一个缺陷：无法对时间序列上的变化进行建模，然而，样本出现的时间顺序对于自然语言处理、语音识别等应用很重要；RNN解决了样本的处理在各个时刻独立的问题，可以对时间序列上的变化进行建模，深度是时间上的长度。神经元的输出可以在下一个时间戳直接作用到自身。即，某一层某一时刻神经元的输入，除了上一层神经元在该时刻的输出外，还有本身在上一时刻的输出。

11.RNN的缺点
时间轴上的“梯度消失”；
解决办法：长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，防止梯度消失。

12.LSTM
核心：模仿一种细胞状态，类似传送带思想，直接在整个链上运行，只有一些少量的线性交互，信息在上面保持不变。利用一种“门”的结构来去除或增加信息到细胞状态的能力，有三个门。门：让信息选择通过的方法，包括sigmoid神经网络层和一个点乘操作。
第一步：忘记门层：决定从细胞状态中丢弃什么信息。读取本层的输入和上一层的输出，输出一个0到1之间的数值给每个细胞状态。
第二步：确定什么样的信息被存放在细胞状态中，包含两个部分：1）sigmoid“输入门层”，决定什么值将要更新。2）tanh层，创建一个新的候选值向量。会被加到状态中。
第三步：更新细胞状态。基于细胞状态确定输出什么值。

13.RNN、LSTM、GRU区别
RNN引入了循环的概念，但是在实际过程中却出现了初始信息随时间消失的问题，即长期依赖（Long-Term Dependencies）问题，所以引入了LSTM。
LSTM：因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。推导forget gate，input gate，cell state， hidden information等因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸的变化是关键，下图非常明确适合记忆：
![image](https://user-images.githubusercontent.com/33319687/132277107-24fcc657-f595-4877-ad8c-659cae5ba7eb.png)
GRU是LSTM的变体，将忘记门和输入们合成了一个单一的更新门：
![image](https://user-images.githubusercontent.com/33319687/132277138-29b679c2-6a32-4f15-a1ea-9831a0385025.png)

14.LSTM防止梯度弥散和爆炸
LSTM用加和的方式取代了乘积，使得很难出现梯度弥散。但是相应的更大的几率会出现梯度爆炸，但是可以通过给梯度加门限解决这一问题.

15.深度学习中的超参数
第一重要的：learning rate
第二重要的：mimibatch size，units，
第三重要的：layer， learning rate dacay

16.如何初始化参数
参考blog：深度学习中神经网络的几种权重初始化方法http://https//blog.csdn.net/u012328159/article/details/80025785
（1）把w初始化为0；
在神经网络中，每一层的神经元学到的东西都是一样的（输出是一样的），不能初始化为0
（2）对w随机初始化；
随机初始化的缺点，np.random.randn()是一个均值为0，方差为1的高斯分布中采样。
当神经网络的层数增多时，会发现越往后面的层的激活函数（使用tanH）的输出值几乎都接近于0。
（3）Xavier initialization；
为了解决随机初始化的问题提出来的另一种初始化方法，这种初始化方法能尽可能的让输入和输出服从相同的分布，避免后面层的激活函数的输出值趋向于0。但是对于ReLU作为激活函数的模型，也会出现上面问题。
（4）He initialization；
解决了（3）中使用Relu的问题。

17.CNN最成功的应用是在CV，为什么NLP和Speech的很多问题也可以用CNN解出来？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？
相关性：存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。
CNN通过局部感知、权值共享、池化操作、多层次结构抓住了这个共性。局部感知使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度；池化操作和多层次结构一起，实现了数据的降维，将低层次的特征组合成高层次的特征。

18.什么时候用local-conv？什么时候用全卷积（每一个点用同一个filter）？
在不同的区域有不同的特征分布时，适合用local-Conv；
当数据集具有全局的局部特征分布时，也就是说局部特征之间有较强的相关性，适合用全卷积。

19.什么样的资料不适合用深度学习？
1）数据集太小，因为神经网络有效的关键就是大量的数据，有大量的参数需要训练，少量的数据不能充分训练参数。
2）数据集没有局部相关性。目前深度学习应用的领域主要是图像、语音、自然语言处理，这些领域的共性就是局部相关性。例如：图像中的像素组成物体，语音中的音位组成单词，文本数据中的单词组成句子，而深度学习的本质就是学习局部低层次的特征，然后组合低层次的特征成高层次的特征，得到不同特征之间的空间相关性

20.常用cnn及介绍，每一个经典模型的创新点
different_cnn

21.有哪些（调参）技巧
22.Dropout的为什么可以解决过拟合？
23.Batch-normalization的思想是什么？
参考博客：https://blog.csdn.net/qq_34896915/article/details/73565045
https://blog.csdn.net/woaidapaopao/article/details/77806273
